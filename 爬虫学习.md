## 通用代码框架
``` python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()    #如果状态码不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except :
        return "产生异常"

if __name__ == "__main__":
    url = "https://www.baidu.com"
    print(getHTMLText(url))
```

## HTTP协议及Requests库方法
| 方法               | 说明                                           |
| ------------------ | ---------------------------------------------- |
| requests.request() | 构造一个请求，支撑以下各方法的基础方法         |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET        |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的HEAD     |
| requests.post()    | 向HTML网页提交POST请求的方法，对应于HTTP的POST |
| requests.put()     | 向HTML网页提交PUT请求的方法，对应于HTTP的PUT   |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH  |
| requests.delete()  | 向HTML页面提交删除请求，对应于HTTP的DELETE     |

* HTTP协议
  * 超文本传输协议。
  * HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。
  * HTTP协议采用URL作为定位网络资源的标识。
  * URL格式 `http://host[:port][path]`
    * host：合法的Internet主机域名或IP地址
    * port：端口号，缺省端口号为80
    * path：请求资源的路径
  * HTTP URL的理解：
    * URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源。  

## Requests库主要方法解析

#### requests.request(`method`, `url`, `**kwargs`)
* method：请求方式，对应get/put/post等7种
* url：拟获取页面的url链接
* **kwargs：控制访问的参数，共13个
  * params：字典或字节序列，作为参数增加到url中
  * data：字典、字节序列或文件对象，作为Request的内容
  * json：JSON格式的数据，作为Request的内容
  * headers：字典，HTTP定制头
    * hd = {'user-agent': 'Chrome/10'}
    * r = requests.request('POST', 'http://python123.io/ws', headers=hd)
  * cookies：字典或CookieJar, Request中的cookie
  * auth：元组，支持HTTP认证功能
  * files：字典类型，传输文件
  * timeout：设定超时时间，秒为单位
  * proxies：字典类型，设定访问代理服务器，可以增加登录认证
    * pxs = {'http':'http://user:pass@10.10.10.1:1234}
    * r = requests.request('GET','http://www.baidu.com', proxies=pxs)
  * allow_redirects：重定向开关，默认True
  * stream：获取内容立即下载开发，默认True
  * verify：认证SSL正式开关，默认True
  * cert：本地SSL证书路径


## 网络爬虫引发的问题

| 小规模，数据量小<br>爬取速度不敏感<br>Requests库 | 中规模，数据规模较大<br>爬取速度敏感<br>Scrapy库 | 大规模，搜索引擎<br>爬取速度关键<br>定制开发 |
| ------------------------------------------------ | ------------------------------------------------ | -------------------------------------------- |
| 爬取网页 玩转网页                                | 爬取网站 爬取系列网站                            | 爬取全网                                     |

<br>

#### 网络爬虫的法律风险
#### 服务器上的数据有产权归属

<br>

* 骚扰问题
* 法律问题
* 隐私泄露问题

## 网络爬虫的限制

* 来源审查：判断User-Agent进行限制
  * 检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。
* 发布公告：Robots协议
  * 告知所有爬虫网站的爬取策略，要求爬虫遵守。

## Robots协议
* `Robots Exclusion Standard` 网络爬虫排除标准
* 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。
* 形式：在网站根目录下的robots.txt文件。

## Robots协议的遵守方式

* 网络爬虫：自动或人工识别robots.txt，再进行内容爬取。
* 约束性：Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。

| 访问量很小：可以遵守<br>访问量较大：建议遵守 | 非商业且偶尔：建议遵守<br>商业利益：必须遵守 | 必须遵守 |
| -------------------------------------------- | -------------------------------------------- | -------- |
| 爬取网页 玩转网页                            | 爬取网站 爬取系列网站                        | 爬取全网 |


## 亚马逊案例

``` python
import requests
url = "https://www.amazon.cn/gp/product/B01M8L5Z3Y"
try:
  kv = {'user-agent':'Mozilla/5.0'}
  r = requests.get(url,headers=kv)
  r.raise_for_status()
  r.encoding = r.apparent_encoding
  print(r.text[1000:2000])
except:
  print("爬取失败")
```

* 伪装成浏览器

## 图片爬取
``` python

import requests
import os

url = "http://sfsf.com/sfsf.jpg"
root = "D://pics//"
path = root + url.split('/')[-1]
try:
  if not os.path.exists(root):
    os.mkdir(root)
  if not os.path.exists(path):
    r = requests.get(url)
    with open(path, 'wb') as f:
      f.write(r.content)
      f.close()
      print("文件保存成功")
  else:
    print("文件已存在")
except:
  print("爬取失败")

```

## Beautiful Soup
* 安装  
> pip install beautifulsoup4
---
例子：
> import requests

> r = requests.get("http://python123.io/ws/demo.html")

> demo = r.text

> from bs4 import BeautifulSoup  # 类

> soup = BeautifulSoup(demo, "html.parser") # 解释器

> print(soup.prettify())

## BeautifulSoup库的基本元素

Beautiful Soup库是解析、遍历、维护“标签书”的功能库。

### Beautiful Soup库的引用
* Beautiful Soup库，也叫beautifulsoup4或bs4。
> from bs4 import BeautifulSoup
从bs4库里引用Beautiful Soup类、注意大写
* BeautifulSoup对应一个HTML/XML文档的全部内容。
> soup = BeautifulSoup(open("D://demo.html), "html.parser")

### Beautiful Soup库解析器

| 解析器           | 使用方法                         | 条件                 |
| ---------------- | -------------------------------- | -------------------- |
| bs4的HTML解析器  | BeautifulSoup(mk, 'html.parser') | 安装bs4库            |
| lxml的HTML解析器 | BeautifulSoup(mk, 'html.parser') | pip install lxml     |
| lxml的XML解析器  | BeautifulSoup(mk, 'xml')         | pip install lxml     |
| html5lib的解析器 | BeautifulSoup(mk, 'html5lib')    | pip install html5lib |

### Beautiful Soup类的基本元素

| 基本要素       | 说明                                                       |
| -------------- | ---------------------------------------------------------- |
| Tag            | 标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾    |
| Name           | 标签的名字，`<p>...</p>`的名字是'p'，格式：`<tag>.name`    |
| Attributes     | 标签的属性，字典形式组织，格式：`<tag>.attrs`              |
| NavigableSring | 标签内非属性字符串，<>...</>中字符串，格式：`<tag>.string` |
| Comment        | 标签内字符串的注释部分，一种特殊的Comment类型              |


## 基于bs4库的HTML遍历方法

### 标签树的下行遍历

| 属性         | 说明                                                    |
| ------------ | ------------------------------------------------------- |
| .contents    | 子节点的列表，将<tag>所有儿子节点存入列表               |
| .children    | 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点 |
| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历      |

### 标签树的上行遍历

| 属性     | 说明                                         |
| -------- | -------------------------------------------- |
| .parent  | 节点的父亲标签                               |
| .parents | 节点先辈标签的迭代类型，用于循环遍历先辈节点 |

### 标签树的平行遍历

| 属性               | 说明                                                 |
| ------------------ | ---------------------------------------------------- |
| .next_sibling      | 返回按照HTML文本顺序的下一个平行节点标签             |
| .previous_sibling  | 返回按照HTML文本顺序的上一个平行节点标签             |
| .next_siblings     | 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 |
| .previous_siblings | 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 |









