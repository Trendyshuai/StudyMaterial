## 通用代码框架
``` python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()    #如果状态码不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except :
        return "产生异常"

if __name__ == "__main__":
    url = "https://www.baidu.com"
    print(getHTMLText(url))
```

## HTTP协议及Requests库方法
| 方法 | 说明 |
|---|---|
| requests.request() | 构造一个请求，支撑以下各方法的基础方法 |
| requests.get() | 获取HTML网页的主要方法，对应于HTTP的GET |
| requests.head() | 获取HTML网页头信息的方法，对应于HTTP的HEAD
| requests.post() | 向HTML网页提交POST请求的方法，对应于HTTP的POST
| requests.put() | 向HTML网页提交PUT请求的方法，对应于HTTP的PUT
| requests.patch() | 向HTML网页提交局部修改请求，对应于HTTP的PATCH
| requests.delete() | 向HTML页面提交删除请求，对应于HTTP的DELETE

* HTTP协议
  * 超文本传输协议。
  * HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。
  * HTTP协议采用URL作为定位网络资源的标识。
  * URL格式 `http://host[:port][path]`
    * host：合法的Internet主机域名或IP地址
    * port：端口号，缺省端口号为80
    * path：请求资源的路径
  * HTTP URL的理解：
    * URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源。  

## Requests库主要方法解析

#### requests.request(`method`, `url`, `**kwargs`)
* method：请求方式，对应get/put/post等7种
* url：拟获取页面的url链接
* **kwargs：控制访问的参数，共13个
  * params：字典或字节序列，作为参数增加到url中
  * data：字典、字节序列或文件对象，作为Request的内容
  * json：JSON格式的数据，作为Request的内容
  * headers：字典，HTTP定制头
    * hd = {'user-agent': 'Chrome/10'}
    * r = requests.request('POST', 'http://python123.io/ws', headers=hd)
  * cookies：字典或CookieJar, Request中的cookie
  * auth：元组，支持HTTP认证功能
  * files：字典类型，传输文件
  * timeout：设定超时时间，秒为单位
  * proxies：字典类型，设定访问代理服务器，可以增加登录认证
    * pxs = {'http':'http://user:pass@10.10.10.1:1234}
    * r = requests.request('GET','http://www.baidu.com', proxies=pxs)
  * allow_redirects：重定向开关，默认True
  * stream：获取内容立即下载开发，默认True
  * verify：认证SSL正式开关，默认True
  * cert：本地SSL证书路径


## 网络爬虫引发的问题

| 小规模，数据量小<br>爬取速度不敏感<br>Requests库 | 中规模，数据规模较大<br>爬取速度敏感<br>Scrapy库 | 大规模，搜索引擎<br>爬取速度关键<br>定制开发 |
|---|---|---|
| 爬取网页 玩转网页 | 爬取网站 爬取系列网站 | 爬取全网 |

<br>

#### 网络爬虫的法律风险
#### 服务器上的数据有产权归属

<br>

* 骚扰问题
* 法律问题
* 隐私泄露问题

## 网络爬虫的限制

* 来源审查：判断User-Agent进行限制
  * 检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。
* 发布公告：Robots协议
  * 告知所有爬虫网站的爬取策略，要求爬虫遵守。

## Robots协议
* `Robots Exclusion Standard` 网络爬虫排除标准
* 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。
* 形式：在网站根目录下的robots.txt文件。

## Robots协议的遵守方式

* 网络爬虫：自动或人工识别robots.txt，再进行内容爬取。
* 约束性：Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。

| 访问量很小：可以遵守<br>访问量较大：建议遵守 | 非商业且偶尔：建议遵守<br>商业利益：必须遵守 | 必须遵守
|---|---|---|
| 爬取网页 玩转网页 | 爬取网站 爬取系列网站 | 爬取全网


## 亚马逊案例

``` python
import requests
url = "https://www.amazon.cn/gp/product/B01M8L5Z3Y"
try:
  kv = {'user-agent':'Mozilla/5.0'}
  r = requests.get(url,headers=kv)
  r.raise_for_status()
  r.encoding = r.apparent_encoding
  print(r.text[1000:2000])
except:
  print("爬取失败")
```

* 伪装成浏览器

## 图片爬取
``` python

import requests
import os

url = "http://sfsf.com/sfsf.jpg"
root = "D://pics//"
path = root + url.split('/')[-1]
try:
  if not os.path.exists(root):
    os.mkdir(root)
  if not os.path.exists(path):
    r = requests.get(url)
    with open(path, 'wb') as f:
      f.write(r.content)
      f.close()
      print("文件保存成功")
  else:
    print("文件已存在")
except:
  print("爬取失败")

```