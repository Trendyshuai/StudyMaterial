# <center>深度学习入门：基于Python的理论与实践</center>

@author Shine.Lee \
@created 2021-6-6 18:34

# 第一章 Python入门
略。
# 第二章 感知机

感知机是一种算法。1957年美国学者Frank Rosenblatt提出来的。感知机是神经网络（深度学习）的起源算法。

学习感知机的构造也就是学习通向神经网络的和深度学习的一种重要思想。


> 2.1 感知机是什么？

感知机接收多个输入信号，输出一个信号。

感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。

神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活”。

这里将这个界限值称为阈值θ。

> 2.2 简单逻辑电路

> 2.2.1 与门

| x1  | x2  | y   |
| --- | --- | --- |
| 0   | 0   | 0   |
| 1   | 0   | 0   |
| 0   | 1   | 0   |
| 1   | 1   | 1   |

> 2.2.2 与非门和或门

| x1  | x2  | y   |
| --- | --- | --- |
| 0   | 0   | 1   |
| 1   | 0   | 1   |
| 0   | 1   | 1   |
| 1   | 1   | 0   |

把与门的参数值的符号取反，就可以实现与非门。

| x1  | x2  | y   |
| --- | --- | --- |
| 0   | 0   | 0   |
| 1   | 0   | 1   |
| 0   | 1   | 1   |
| 1   | 1   | 1   |


> 2.3 感知机的实现

> 2.3.1 简单的实现

``` python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```
``` python
AND(0, 0) # 输出0
AND(1, 0) # 输出0
AND(0, 1) # 输出0
AND(1, 1) # 输出1
```

> 2.3.2 导入权重和偏置

把阈值θ换成偏置-b

> 2.3.3 使用权重和偏置的实现

``` python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

> 2.4 感知机的局限性

使用感知机可以实现与门、与非门、或门三种逻辑电路。现在我们来考虑一下异或门（XOR gate）。

> 2.4.1 异或门

仅当x1或x2中的一方为1时，才会输出1。

| x1  | x2  | y   |
| --- | --- | --- |
| 0   | 0   | 0   |
| 1   | 0   | 1   |
| 0   | 1   | 1   |
| 1   | 1   | 0   |

感知机是无法实现这个异或门的。

> 2.4.2 线性和非线性

感知机的局限性就在于它只能表示由一条直线分割的空间。

由曲线分割而成的空间称为非线性空间，由直线分割而成的空间称为线性空间。

> 2.5 多层感知机

感知机的绝妙之处在于它可以“叠加层”来表示异或门。

> 2.5.1 已有门电路的组合

`s1`代表非的输出结果。

`s2`代表或的输出结果。

| x1  | x2  | s1  | s2  | y   |
| --- | --- | --- | --- | --- |
| 0   | 0   | 1   | 0   | 0   |
| 1   | 0   | 1   | 1   | 1   |
| 0   | 1   | 1   | 1   | 1   |
| 1   | 1   | 0   | 1   | 0   |

异或

> 2.5.2 异或门的实现

``` python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```

``` python 
XOR(0, 0) # 输出0
XOR(1, 0) # 输出1
XOR(0, 1) # 输出1
XOR(1, 1) # 输出0
```

异或门是多层感知机。

x层感知机，一共有x层权重，就称x层感知机。

> 2.6 从与非门到计算机

感知机通过叠加层能够进行非线性的表示，理论上还可以表示计算机进行的处理。

> 2.7 小结

* 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。
* 感知机将权重和偏置设定为参数。
* 使用感知机可以表示与门和或门等逻辑电路。
* 异或门无法通过单层感知机来表示。
* 使用2层感知机可以表示异或门。
* 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。
* 多层感知机（在理论上）可以表示计算机。

# 第三章 神经网络

对于上一章来说，对于复杂的函数，感知机也隐含着能够表示它的可能性。但是，设定权重的工作，只能由人工进行。

神经网络的出现，就可以自动地从数据中学习到合适的权重参数。

> 3.1 从感知机到神经网络

神经网络和上一章介绍的感知机有很多共同点。这里，我们主要以两者的差异为中心，来介绍神经网络的结构。

> 3.1.1 神经网络的例子

中间层有时也成为隐藏层。

> 3.1.2 复习感知机

```
    ┍ 0 (b + w1x1 + w2x2) <= 0
y = │
    └ 1 (b + w1x1 + w2x2) > 0
```

y=h(b+w1x1+w2x2)

> 3.1.3 激活函数登场

h(x)函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数。

激活函数的作用在于决定如何来激活输入信号的总和。

激活函数是连接感知机和神经网络的桥梁。

> 3.2 激活函数

阶跃函数是指一旦输出超过阈值，就切换输出的函数。

感知机使用了阶跃函数作为激活函数。

如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。

> 3.2.1 sigmoid函数

神经网络中经常使用的一个激活函数就是sigmoid函数。

h(x) = 1/(1+exp(-x))

* exp(-x)表示以e为底的-x次幂。

感知机和神经网络的主要区别就在于这个激活函数。

> 3.2.2 阶跃函数的实现

``` python
def step_function(x):
    if x > 0:
        return 1
    else:
        return 0
```

参数x只能接受实数（浮点数），为了后面的操作，我们把它修改为支持NumPy数组的实现。

``` python
def step_function(x):
    y = x > 0
    return y.astype(np.int)
```

astype()方法转换Numpy数组的类型。

> 3.2.3 阶跃函数的图形

``` python
import numpy as npm
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x > 0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # 指定y轴的范围
plt.show()
```

np.arange(-5.0, 5.0, 0.1)代表范围从-5.0到5.0，单位为0.1生成Numpy数组。


> 3.2.4 sigmoid函数的实现

``` python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
x为Numpy数组时，结果也能正确计算。
之所以sigmoid支持Numpy数组功能，秘密在于Numpy的广播功能。

* sigmoid函数的图形

``` python 
x = np.arrange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

> 3.2.5 sigmoid函数和阶跃函数的比较

不同点

* 平滑性不同。sigmoid函数比较平滑，阶跃函数以0为界，输出发生急剧变化。
  * sigmoid函数的平滑对神经网络的学习有重要意义。
* 返回值范围不同。阶跃函数只能返回0和1，而sigmoid函数能返回0.731、0.880等实数。
  * 也就是说，感知机神经元之间流动的是二元信号，神经网络中流动的是连接的实数值信号。

共同点

* 输入小数时，都接近0；输出大数时，都接近1。
* 无论输入多大多小的数都在0-1之间。

> 3.2.6 非线性函数

阶跃函数和sigmoid函数还有一个其他的共同点，就是它们都是非线性函数。

sigmoid函数是一条曲线，阶跃函数是像阶梯一样的折线。两者都是非线性函数。

* 线性：是指量与量之间按比例，成直线的关系。输出值是输入值常数倍的函数叫做线性函数。eg. h(x)=cx

* 神经网络的激活函数必须使用非线性函数。
* 如果使用线性函数的话，加深神经网络的层数就没有意义了。

* eg. 激活函数为h(x) = cx
  * y(x) = h(h(h(x)))；相当于 y(x) = c * c * c * x
  * 可以直接写成 y(x) = ax；a为c的立方

> 3.2.7 ReLU函数

在神经网络发展的历史上，sigmoid函数很早就被使用了。而最近则主要使用ReLU（Rectified Linear Unit）函数。

ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。

```
    ┍ x (x > 0)
y = │
    └ 0 (x <= 0)
```

ReLU实现

``` python
def relu():
    return np.maximum(0, x)
```

np.maximum()就是max函数，挑大的输出。

> 3.3 多维数组的运算
掌握Numpy多维数组的运算，就可以高效地实现神经网络。

> 3.3.1 多维数组

多维数组就是“数字的集合”。

``` python
>>> import numpy as np
>>> A = np.array([1, 2, 3, 4])
>>> print(A)
[1 2 3 4]
>>> np.ndim(A)
1
>>> A.shape
(4,)
>>> A.shape[0]
4
```

* 数组的维数，可以通过np.dim()获得。
* 数组的形状可以通过shape获得。
* A.shape获得的结果是一个元组（tuple）。

下面生成一个二维数组:
``` python
>>> B = np.array([[1,2], [3,4], [5,6]])
>>> print(B)
[[1 2]
 [3 4]
 [5 6]]
>>> np.ndim(B)
2
>>> B.shape
(3, 2)
```

二维数组也成为矩阵（matrix）。

> 3.3.2 矩阵乘法




